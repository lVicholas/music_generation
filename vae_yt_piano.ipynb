{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cf725e",
   "metadata": {},
   "source": [
    "# Training a Variational Autoencoder to generate classical music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7dbf7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import pypianoroll\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import scipy.sparse\n",
    "from scipy.sparse import coo_matrix, save_npz, load_npz\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Conv1D, Conv1DTranspose\n",
    "from tensorflow.keras.layers import Conv3D, Conv3DTranspose\n",
    "from tensorflow.keras.layers import ConvLSTM1D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import GRU, LSTM\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Concatenate, concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization as BatchNorm\n",
    "from tensorflow.keras.layers import ReLU as Relu\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import math\n",
    "\n",
    "import datetime\n",
    "from IPython import display\n",
    "\n",
    "import pygame\n",
    "import time\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30bc64c",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a95f6f",
   "metadata": {},
   "source": [
    "The data used in this notebook is the MAESTRO dataset, which consists of about 1,200 MIDI files with a single track of a piano. Prior to the execution of this notebook, I converted these MIDI files into piano rolls which were then saved as COO scipy sparse matrices. Although the COO matrices take up more space than the MIDI files, I found that it was better for performance to save the piano rolls and then load them into memory for training, rather than load the MIDI files into memory and then have to constantly convert them to piano rolls while training, or to continously get training samples from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b71b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model directories\n",
    "MODELS_ROOT_DIR = 'C:/_local/py/classical_music_vae/models/'\n",
    "CVAE_DIR = MODELS_ROOT_DIR + 'cvae/'\n",
    "MELODY_PREDICTION_MODEL_DIR = MODELS_ROOT_DIR + 'melody_predictor/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9fff5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAESTRO directories\n",
    "MAESTRO_DATA_ROOT_DIR = 'C:/_local/data_sets/audio/maestro_v3.0.0_sparse/'\n",
    "MAESTRO_TRAINING_DATA_DIR = MAESTRO_DATA_ROOT_DIR + 'train/'\n",
    "MAESTRO_VALIDATION_DATA_DIR = MAESTRO_DATA_ROOT_DIR + 'validation/'\n",
    "MAESTRO_TESTING_DATA_DIR = MAESTRO_DATA_ROOT_DIR + 'test/'\n",
    "\n",
    "# Youtube Piano directories\n",
    "YT_PIANO_DATA_ROOT_DIR = 'C:/_local/data_sets/audio/youtube_piano/piano_rolls/'\n",
    "\n",
    "# Output directories\n",
    "OUTPUTS_ROOT_DIR = 'C:/_local/py/classical_music_vae/outputs/'\n",
    "NOTE_OUTPUT_DIR = OUTPUTS_ROOT_DIR + 'notes/'\n",
    "SIMPLE_SEQUENCE_OUTPUT_DIR = OUTPUTS_ROOT_DIR + 'simple_sequences/'\n",
    "MELODY_OUTPUT_DIR = OUTPUTS_ROOT_DIR + 'melodies/'\n",
    "TEMP_OUTPUT_PATH = OUTPUTS_ROOT_DIR + 'temp.mid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b820c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "YT_PIANO_OR_MAESTRO = True # <-- True for YT_Piano, False for Maestro\n",
    "\n",
    "if YT_PIANO_OR_MAESTRO:\n",
    "    \n",
    "    number_to_subset = 1200 # <-- Only use load this many piano rolls \n",
    "    subset_test_size = 200 # <-- Allocate this many for testing\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    piano_roll_paths = np.array([YT_PIANO_DATA_ROOT_DIR+f for f in os.listdir(YT_PIANO_DATA_ROOT_DIR)])\n",
    "    piano_roll_paths_subset = np.random.choice(piano_roll_paths, size=number_to_subset)\n",
    "    train_ids, test_ids = train_test_split(np.arange(len(piano_roll_paths_subset)), test_size=subset_test_size)\n",
    "    \n",
    "    TRAINING_PIANO_ROLL_PATHS = piano_roll_paths[train_ids]\n",
    "    TESTING_PIANO_ROLL_PATHS = piano_roll_paths[test_ids]\n",
    "    \n",
    "else:\n",
    "    \n",
    "    TRAINING_PIANO_ROLL_PATHS = [TRAINING_DATA_DIR+f for f in os.listdir(MAESTRO_TRAINING_DATA_DIR)]\n",
    "    TESTING_PIANO_ROLL_PATHS = [TESTING_DATA_DIR+f for f in os.listdir(MAESTRO_TESTING_DATA_DIR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b5e2131",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 64 # <-- length of input / target sequences in 1/100 seconds\n",
    "LATENT_DIM = 64 # <-- Number of dimensions to encode piano rolls into\n",
    "BATCH_SIZE = 32 # <-- Batch size for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5e5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for converting scipy-sparse matrices to tf.sparse.SparseTensor\n",
    "def scipy_sparse_to_sparse_tensor(scipy_sparse):\n",
    "    \n",
    "    indices = np.mat([scipy_sparse.row, scipy_sparse.col]).transpose()\n",
    "    return tf.cast(tf.sparse.SparseTensor(indices, \n",
    "                                          scipy_sparse.data, \n",
    "                                          scipy_sparse.shape),\n",
    "                    dtype=tf.float32\n",
    "                   )\n",
    "\n",
    "def list_of_scipy_sparse_to_list_sparse_tensor(list_scipy_sparse):\n",
    "    return [scipy_sparse_to_sparse_tensor(s) for s in list_scipy_sparse]\n",
    "\n",
    "def piano_roll_path_to_sparse_tensor(piano_roll_path):\n",
    "    \n",
    "    s = load_npz(piano_roll_path)   \n",
    "    return scipy_sparse_to_sparse_tensor(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea3676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_piano_rolls_nonzero_mean_std(piano_roll_paths):\n",
    "    \n",
    "    nz_sums = []\n",
    "    total_nnz = 0\n",
    "    \n",
    "    sparse = None\n",
    "        \n",
    "    for prp in piano_roll_paths:\n",
    "        sparse = load_npz(prp)\n",
    "        nz_sums.append(sparse.data.sum())\n",
    "        total_nnz += sparse.nnz\n",
    "    \n",
    "    mean = (np.array(nz_sums) / total_nnz).sum()\n",
    "    \n",
    "    def get_std_disc(x, u):\n",
    "        return (((x - u)**2)/total_nnz).sum()\n",
    "    \n",
    "    nz_std_disc_sum = 0\n",
    "    for prp in piano_roll_paths:\n",
    "        sparse = load_npz(prp)\n",
    "        nz_std_disc_sum += get_std_disc(sparse.data, mean)\n",
    "    \n",
    "    std = nz_std_disc_sum ** (.5)\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "def get_sparse_mats_mean_std(sparse_matrices):\n",
    "    \n",
    "    sums = []\n",
    "    total_size = 0\n",
    "        \n",
    "    for s in sparse_matrices:\n",
    "        sums.append(s.A.sum())\n",
    "        total_size += s.shape[0]*s.shape[1]\n",
    "    \n",
    "    mean = (np.array(sums) / total_size).sum()\n",
    "    \n",
    "    def get_std_disc(x, u):\n",
    "        return (((x - u)**2)/total_size).sum()\n",
    "    \n",
    "    std_disc_sum = 0\n",
    "    for s in sparse_matrices:\n",
    "        std_disc_sum += get_std_disc(s.A.reshape(-1,1), mean)\n",
    "    \n",
    "    std = nz_std_disc_sum ** (.5)\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "def normal_sparse_matrix(sparse_matrix, mean, std, inplace):\n",
    "    \n",
    "    if inplace:\n",
    "        sparse_matrix.data = (sparse_matrix.data - mean) / std\n",
    "    else:\n",
    "        sparse_matrix_new = sparse_matrix\n",
    "        sparse_matrix_new.data = (sparse_matrix_new.data - mean) / std\n",
    "        return sparse_matrix_new\n",
    "\n",
    "def normalize_list_of_sparse_matrices(sparse_matrices):\n",
    "    \n",
    "    mean, std = get_sparse_mats_nonzero_mean_std(sparse_matrices)\n",
    "    \n",
    "    for s in sparse_matrices:    \n",
    "        s.data = (s.data - mean) / std\n",
    "        \n",
    "    return sparse_matrices\n",
    "\n",
    "def piano_roll_paths_to_norm_sparse_tensors(piano_roll_paths):\n",
    "    \n",
    "    mean, std = get_piano_rolls_nonzero_mean_std(piano_roll_paths)\n",
    "    \n",
    "    sparse_tensors = []\n",
    "    for prp in piano_roll_paths:\n",
    "        \n",
    "        sparse_matrix = load_npz(prp)\n",
    "        normal_sparse_matrix(sparse_matrix, mean, std, True)\n",
    "        sparse_tensors.append(scipy_sparse_to_sparse_tensor(sparse_matrix))\n",
    "        \n",
    "    return sparse_tensors\n",
    "\n",
    "def piano_roll_paths_to_scaled_sparse_tensors(piano_roll_paths):\n",
    "    \n",
    "    sparse_tensors = []\n",
    "    for prp in piano_roll_paths:\n",
    "        \n",
    "        sparse_matrix = load_npz(prp)\n",
    "        sparse_matrix.data[sparse_matrix.data > 127] = 127.\n",
    "        sparse_matrix.data = sparse_matrix.data / 127.\n",
    "        \n",
    "        sparse_tensors.append(scipy_sparse_to_sparse_tensor(sparse_matrix))\n",
    "        \n",
    "    return np.array(sparse_tensors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202b69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIANO_ROLLS_TRAIN = piano_roll_paths_to_scaled_sparse_tensors(TRAINING_PIANO_ROLL_PATHS)\n",
    "PIANO_ROLLS_TEST = piano_roll_paths_to_scaled_sparse_tensors(TESTING_PIANO_ROLL_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dacfec7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training piano rolls: 1000\n",
      "testing piano rolls: 200\n"
     ]
    }
   ],
   "source": [
    "print(f'training piano rolls: {PIANO_ROLLS_TRAIN.shape[0]}')\n",
    "print(f'testing piano rolls: {PIANO_ROLLS_TEST.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386741ae",
   "metadata": {},
   "source": [
    "## Piano Roll helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "703284b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def piano_roll_to_pretty_midi(piano_roll, fs=100):\n",
    "    \n",
    "    notes, frames = piano_roll.shape\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(program=0)\n",
    "\n",
    "    # pad 1 column of zeros so we can acknowledge inital and ending events\n",
    "    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n",
    "\n",
    "    # use changes in velocities to find note on / note off events\n",
    "    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n",
    "\n",
    "    # keep track on velocities and note on times\n",
    "    prev_velocities = np.zeros(notes, dtype=int)\n",
    "    note_on_time = np.zeros(notes)\n",
    "\n",
    "    for time, note in zip(*velocity_changes):\n",
    "        # use time + 1 because of padding above\n",
    "        velocity = piano_roll[note, time + 1]\n",
    "        time = time / fs\n",
    "        if velocity > 0:\n",
    "            if prev_velocities[note] == 0:\n",
    "                note_on_time[note] = time\n",
    "                prev_velocities[note] = velocity\n",
    "        else:\n",
    "            pm_note = pretty_midi.Note(\n",
    "                velocity=prev_velocities[note],\n",
    "                pitch=note,\n",
    "                start=note_on_time[note],\n",
    "                end=time)\n",
    "            instrument.notes.append(pm_note)\n",
    "            prev_velocities[note] = 0\n",
    "    pm.instruments.append(instrument)\n",
    "    return pm    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef5c5351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_piano_roll(piano_roll_array, buffer_time=0, threshold=.3, temp_path=TEMP_OUTPUT_PATH):\n",
    "    \n",
    "    if isinstance(piano_roll_array, tf.Tensor):\n",
    "        piano_roll = piano_roll_array.numpy().copy().squeeze()\n",
    "    else:\n",
    "        piano_roll = piano_roll_array.copy().squeeze()\n",
    "        \n",
    "    assert len(piano_roll.shape) == 2, 'piano roll has incompatible shape'\n",
    "    \n",
    "    if piano_roll.max() <= 1.2:\n",
    "        piano_roll *= 127.\n",
    "\n",
    "    piano_roll[piano_roll < 127 * threshold] = 0\n",
    "    piano_roll[piano_roll > 127] = 127\n",
    "    piano_roll = piano_roll.round().astype('uint8')\n",
    "            \n",
    "    midi = piano_roll_to_pretty_midi(piano_roll)\n",
    "    midi.write(temp_path)\n",
    "    \n",
    "    sleep_time = piano_roll.shape[-1] / 100 + buffer_time\n",
    "    \n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(temp_path)\n",
    "    \n",
    "    pygame.mixer.music.play()\n",
    "    time.sleep(sleep_time)\n",
    "    \n",
    "    pygame.mixer.music.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5c629fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_samples_from_batch(batch_array, number_of_samples, \n",
    "                            buffer=0, threshold=.3, shuffle=True, temp_path=TEMP_OUTPUT_PATH):\n",
    "    \n",
    "    if isinstance(batch_array, tf.Tensor):\n",
    "        batch = batch_array.numpy().copy()\n",
    "    else:\n",
    "        batch = batch_array.copy()\n",
    "        \n",
    "    # batch_array has shape batch_size x number_of_melodies x 128 x time_steps\n",
    "    #                    or batch_size x 128 x time_steps\n",
    "    \n",
    "    if len(batch.shape) == 3:\n",
    "        batch = np.expand_dims(batch, 1)  \n",
    "        \n",
    "    if shuffle:\n",
    "        steps = np.random.choice(np.arange(batch.shape[0]), size=number_of_samples, replace=False)\n",
    "        steps = sorted(steps)\n",
    "        \n",
    "    else: \n",
    "        steps = range(number_of_samples)\n",
    "                \n",
    "    for s in steps:\n",
    "        \n",
    "        display.clear_output(wait=False)\n",
    "        print(f'sample # {s}')\n",
    "        \n",
    "        piano_roll = batch[s]\n",
    "        piano_roll = [piano_roll[m] for m in range(piano_roll.shape[0])]\n",
    "        piano_roll = np.concatenate(piano_roll, -1)\n",
    "        \n",
    "        play_piano_roll(piano_roll, buffer, threshold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d93576df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_inputs_and_outputs(inputs_array, outputs_array, number_of_samples, \n",
    "                            buffer=0., threshold=.3, shuffle=True, midi_path=TEMP_OUTPUT_PATH):\n",
    "    \n",
    "    if isinstance(inputs_array, tf.Tensor):\n",
    "        inputs = inputs_array.numpy().copy()\n",
    "    else:\n",
    "        inputs = inputs_array.copy()\n",
    "           \n",
    "    if isinstance(outputs_array, tf.Tensor):\n",
    "        outputs = outputs_array.numpy().copy()\n",
    "    else:\n",
    "        outputs = outputs_array.copy()\n",
    "    \n",
    "    # Inputs is ndarray shaped batch_size x number_of_melodies x 128 x sequence_length\n",
    "    # Outputs is ndarray batch_size x 128 x sequence_length   \n",
    "    \n",
    "    if len(inputs.shape) == 3:\n",
    "        inputs = np.expand_dims(inputs, 1)\n",
    "        \n",
    "    if shuffle:\n",
    "        possible_sample_indices = range(outputs.shape[0])\n",
    "        steps = sorted(np.random.choice(possible_sample_indices, size=number_of_samples, replace=False))\n",
    "        \n",
    "    else: \n",
    "        steps = range(number_of_samples)\n",
    "    \n",
    "    for s in steps:\n",
    "        \n",
    "        display.clear_output(wait=False)\n",
    "        \n",
    "        sample_input = inputs[s]\n",
    "        sample_input = [sample_input[m] for m in range(sample_input.shape[0])]\n",
    "        sample_input = np.concatenate(sample_input, -1)       \n",
    "        \n",
    "        sample_output = outputs[s]\n",
    "        \n",
    "        print(f'sample #{s}')\n",
    "        \n",
    "        print('input')\n",
    "        play_piano_roll(sample_input, buffer, threshold, temp_path=midi_path)\n",
    "        \n",
    "        print('output')\n",
    "        play_piano_roll(sample_output, buffer, threshold, temp_path=midi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d8130c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_piano_rolls(piano_rolls_raw, zero_buf):\n",
    "    \n",
    "    if isinstance(piano_rolls_raw, tf.Tensor):\n",
    "        piano_rolls = piano_rolls_raw.numpy().copy()\n",
    "            \n",
    "    elif isinstance(piano_rolls_raw, np.ndarray):\n",
    "        piano_rolls = piano_rolls_raw.copy()\n",
    "        \n",
    "    elif isinstance(piano_rolls_raw, list) and isinstance(piano_rolls_raw[0], tf.Tensor):\n",
    "        piano_rolls = [pr.numpy().squeeze().copy() for pr in piano_rolls_raw]\n",
    "        \n",
    "    elif isinstance(piano_rolls_raw, list) and isinstance(piano_rolls_raw[0], np.ndarray):\n",
    "        piano_rolls = [pr.squeeze().copy() for pr in piano_rolls_raw]\n",
    "        \n",
    "    if not isinstance(piano_rolls, list):\n",
    "        \n",
    "        assert len(piano_rolls.shape) == 3, 'piano roll does not have dimension for different sequences'\n",
    "        piano_rolls = [piano_rolls[p] for p in range(piano_rolls.shape[0])]\n",
    "        \n",
    "    assert all(len(pr.shape) == 2 for pr in piano_rolls), 'a piano roll does not have 2 dimensions'\n",
    "    assert all(piano_rolls[0].shape[0] == pr.shape[0] for pr in piano_rolls), 'piano rolls have differing numbers of notes'\n",
    "                \n",
    "    num_notes = piano_rolls[0].shape[0]\n",
    "    \n",
    "    if zero_buf:\n",
    "        return np.concatenate([piano_rolls[p//2] if p % 2 == 0 else np.zeros(shape=(num_notes, 1))\n",
    "                               for p in range(2*len(piano_rolls)-1)], axis=-1)    \n",
    "    \n",
    "    else:\n",
    "        return np.concatenate([piano_rolls[p] for p in range(len(piano_rolls))], axis=-1)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f429ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_piano_roll_to_storage(piano_roll_raw, output_dir, file_name, threshold=.3):\n",
    "    \n",
    "    if isinstance(piano_roll_raw, tf.Tensor):\n",
    "        pr = piano_roll_raw.numpy().copy()\n",
    "    else:\n",
    "        pr = piano_roll_raw.copy()\n",
    "        \n",
    "    assert len(pr.shape) == 2, 'incompatible piano roll shape'\n",
    "        \n",
    "    if pr.max() <= 1.2:\n",
    "        pr *= 127.\n",
    "\n",
    "    pr[pr < 127 * threshold] = 0\n",
    "    pr[pr > 127] = 127\n",
    "    pr = pr.round().astype('uint8')\n",
    "            \n",
    "    midi = piano_roll_to_pretty_midi(pr)\n",
    "    \n",
    "    file_path = output_dir + file_name\n",
    "    midi.write(file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79f1ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_piano_roll_batch_to_storage(piano_roll_batch_raw, output_dir, threshold=.3):\n",
    "    \n",
    "    if isinstance(piano_roll_batch_raw, tf.Tensor):\n",
    "        piano_roll_batch = piano_roll_batch_raw.numpy().copy()\n",
    "        \n",
    "    elif isinstance(piano_roll_batch_raw, np.ndarray):\n",
    "        piano_roll_batch = piano_roll_batch_raw.copy()\n",
    "        \n",
    "    assert len(piano_roll_batch.shape) == 3, 'incompatible shape'\n",
    "    assert piano_roll_batch.shape[1] == 128, 'number of notes != 128'\n",
    "    \n",
    "    for p in range(piano_roll_batch.shape[0]):\n",
    "        \n",
    "        pr = piano_roll_batch[p]\n",
    "        \n",
    "        file_name = f'piano_roll_{p+1}.mid'\n",
    "        write_piano_roll_to_storage(pr, output_dir, file_name, threshold)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1aec29",
   "metadata": {},
   "source": [
    "## Creating TF Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b53be288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoteGenerator:    \n",
    "    \n",
    "    def __init__(self, sparse_tensors, yield_target, sequence_length, seed=None, batch_size=BATCH_SIZE):\n",
    "        \n",
    "        self.sparse_tensors = sparse_tensors\n",
    "        self.num_tensors = len(self.sparse_tensors)\n",
    "        self.yield_target = yield_target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.seed = seed\n",
    "        #self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            sparse_tensor = self.sparse_tensors[np.random.randint(0, self.num_tensors)]\n",
    "\n",
    "            last_start = (sparse_tensor.shape[1] - 2 * self.sequence_length - 3)\n",
    "\n",
    "            note_start = np.random.randint(0, last_start)\n",
    "            note_start -= note_start % 32\n",
    "\n",
    "            note = tf.sparse.slice(sparse_tensor,\n",
    "                                   start=[0, note_start],\n",
    "                                   size=[128, self.sequence_length]\n",
    "                                  )\n",
    "\n",
    "            yield tf.sparse.to_dense(note)\n",
    "            #yield tf.expand_dims(tf.sparse.to_dense(note), axis=-1)                \n",
    "            \n",
    "    def __call__(self):\n",
    "        return self.__iter__()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32d6b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE_OF_GENERATORS = 50\n",
    "\n",
    "train_sub_generators = [NoteGenerator(PIANO_ROLLS_TRAIN[i*DATA_SIZE_OF_GENERATORS:(i+1)*DATA_SIZE_OF_GENERATORS], False, SEQUENCE_LENGTH, i)\n",
    "                        for i in range(1 + PIANO_ROLLS_TRAIN.shape[0] // DATA_SIZE_OF_GENERATORS)\n",
    "                        if i*DATA_SIZE_OF_GENERATORS < PIANO_ROLLS_TRAIN.shape[0]]\n",
    "\n",
    "test_sub_generators = [NoteGenerator(PIANO_ROLLS_TEST[i*DATA_SIZE_OF_GENERATORS:(i+1)*DATA_SIZE_OF_GENERATORS], False, SEQUENCE_LENGTH, i)\n",
    "                        for i in range(1 + PIANO_ROLLS_TEST.shape[0] // DATA_SIZE_OF_GENERATORS)\n",
    "                        if i*DATA_SIZE_OF_GENERATORS < PIANO_ROLLS_TEST.shape[0]]\n",
    "\n",
    "cvae_gen_output_signature = tf.TensorSpec(shape=(128, SEQUENCE_LENGTH))\n",
    "\n",
    "def get_sub_dataset(sub_generator, spec, batch_size, prefetch_size):\n",
    "    \n",
    "    return (tf.data.Dataset\n",
    "            .from_generator(sub_generator, output_signature=spec)\n",
    "            .batch(batch_size, drop_remainder=True)\n",
    "            .prefetch(prefetch_size)\n",
    "           )\n",
    "\n",
    "cvae_train_sub_datasets = [get_sub_dataset(g, cvae_gen_output_signature, BATCH_SIZE, 10)\n",
    "                           for g in train_sub_generators]\n",
    "cvae_test_sub_datasets = [get_sub_dataset(g, cvae_gen_output_signature, BATCH_SIZE, 10)\n",
    "                           for g in test_sub_generators]\n",
    "\n",
    "cvae_train_dataset = tf.data.Dataset.sample_from_datasets(cvae_train_sub_datasets).prefetch(64)\n",
    "cvae_test_dataset = tf.data.Dataset.sample_from_datasets(cvae_test_sub_datasets).prefetch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "883f24d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(333)\n",
    "sample_input = None\n",
    "for x in cvae_test_dataset.take(1):\n",
    "    sample_input = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18b70ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 128, 64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc269856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106.00000244379044"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input.numpy().max()*127."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31eb03",
   "metadata": {},
   "source": [
    "## Training the CVAE to produce notes\n",
    "\n",
    "First, we train the CVAE to learn the distribution of pianoroll values which correspond to notes/chords, and how to generate them. It is trained on batches of 128 x SEQUENCE_LENGTH tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b38f5f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVAE(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, latent_dim, input_shape):\n",
    "        super(MyVAE, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.batch_size, _, self.sequence_length = input_shape\n",
    "        \n",
    "        # Encoder definition\n",
    "        encoder_input = Input(shape=(128, self.sequence_length), name='encoder_input')\n",
    "        encoder_reshape = Reshape(target_shape=(128, self.sequence_length, 1), \n",
    "                                  name='encoder_reshape')(encoder_input)\n",
    "        encoder_conv_1 = Conv2D(filters=64,kernel_size=(4, 4), strides=(4, 4), \n",
    "                                padding='valid', activation='relu', name='encoder_conv2d_1')(encoder_reshape)\n",
    "        encoder_conv_2 = Conv2D(filters=128, kernel_size=(4, 4), strides=(4, 4), \n",
    "                                padding='valid', activation='relu', name='encoder_conv2d_2')(encoder_conv_1)\n",
    "        encoder_conv_3 = Conv2D(filters=256, kernel_size=(8, 2), strides=(8, 2), \n",
    "                                padding='valid', activation='relu', name='encoder_conv2d_3')(encoder_conv_2)\n",
    "        \n",
    "        encoder_flatten = Flatten(name='encoder_flatten')(encoder_conv_3)\n",
    "        \n",
    "        encoder_mean = Dense(self.latent_dim, activation='linear', name='encoder_mu')(encoder_flatten)\n",
    "        encoder_log_sigma = Dense(self.latent_dim, activation='linear', name='encoder_log_sigma')(encoder_flatten)\n",
    "        self.encoder = Model(encoder_input, [encoder_mean, encoder_log_sigma], name='encoder')\n",
    "        downsampled_shape = self.encoder.layers[-4].output_shape[1:]\n",
    "        \n",
    "        # Decoder definition\n",
    "        decoder_input = Input(shape=(self.latent_dim), name='decoder_input')\n",
    "        decoder_dense = Dense(units=512, activation='relu', name='decoder_dense')(decoder_input)\n",
    "        decoder_reshape = Reshape(target_shape=downsampled_shape)(decoder_dense)\n",
    "        decoder_conv_1 = Conv2DTranspose(filters=128, kernel_size=(8, 2), strides=(8, 2), padding='valid', \n",
    "                            activation='relu', name='decoder_conv2dtranspose_1')(decoder_reshape)\n",
    "        decoder_conv_2 = Conv2DTranspose(filters=64, kernel_size=(4, 4), strides=(4, 4), padding='valid', \n",
    "                            activation='relu', name='decoder_conv2dtranspose_2')(decoder_conv_1)\n",
    "        decoder_conv_3 = Conv2DTranspose(filters=1, kernel_size=(4, 4), strides=(4, 4), padding='valid', \n",
    "                            activation='linear', name='decoder_conv2dtranspose_3')(decoder_conv_2)\n",
    "        decoder_output = Reshape(target_shape=(128, self.sequence_length), name='decoder_output')(decoder_conv_3)\n",
    "        \n",
    "        self.decoder = Model(decoder_input, decoder_output, name='decoder')\n",
    "        \n",
    "        # Sigma parameter\n",
    "        self.log_sig_x = tf.Variable(tf.zeros(shape=(128*self.sequence_length)), trainable=True, name='log_sig_x')\n",
    "                \n",
    "    def compile(self, optimizer):\n",
    "        super(MyVAE, self).compile()\n",
    "        \n",
    "        self.optimizer = optimizer \n",
    "        self.encoder.compile(self.optimizer, loss=None)\n",
    "        self.decoder.compile(self.optimizer, loss=None)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def sample_and_reparameterize(self, mu_log_sigma, n_samples):\n",
    "        \n",
    "        #b, k = mu_log_sigma.shape\n",
    "        #k -= 1       \n",
    "        #mu, sigma = mu_log_sigma[:, :-1], tf.exp(mu_log_sigma[:, -1])\n",
    "        #mu = tf.reshape(mu, (b, 1, k))\n",
    "        #sigma = tf.reshape(sigma, (b, 1, 1))\n",
    "        \n",
    "        mu, log_sigma = mu_log_sigma\n",
    "        sigma = tf.exp(log_sigma)\n",
    "        \n",
    "        b, k = mu.shape\n",
    "        \n",
    "        mu = tf.reshape(mu, (b, 1, k))\n",
    "        sigma = tf.reshape(sigma, (b, 1, k))\n",
    "        \n",
    "        eps = tf.random.normal(shape=(b, n_samples, k))\n",
    "        return eps * sigma + mu\n",
    "    \n",
    "    def decode(self, z):\n",
    "                \n",
    "        b, n, k = z.shape        \n",
    "        z = tf.reshape(z, (b*n, -1))\n",
    "        \n",
    "        mu_x = self.decoder(z)        \n",
    "        mu_x = tf.reshape(mu_x, (b, n, -1))\n",
    "        return mu_x\n",
    "    \n",
    "    def sample_latent_vectors(self, x):\n",
    "        \n",
    "        #mu_log_sigma = self.encode(x)\n",
    "        #z = self.sample_and_reparameterize(mu_log_sigma, 1)\n",
    "        \n",
    "        mu_log_sigma = self.encode(x)\n",
    "        z = self.sample_and_reparameterize(mu_log_sigma, 1)\n",
    "        \n",
    "        mu_x = self.decode(z)\n",
    "        return tf.reshape(mu_x, x.shape)\n",
    "        \n",
    "    def log_p_x(self, x, mu_x, sigma_x):\n",
    "        \n",
    "        # x.shape = (b, 128, t)\n",
    "        # mu_x.shape = (b, n, 128, t)\n",
    "        # sigma_x.shape = (b*n)\n",
    "        \n",
    "        b, n = mu_x.shape[:2]\n",
    "        \n",
    "        x = tf.reshape(x, (b, n, -1))\n",
    "        mu_x = tf.reshape(mu_x, (b, n, -1))\n",
    "        \n",
    "        square_error_numerator = tf.square(x - mu_x)\n",
    "        square_error_denominator = 2 * tf.square(sigma_x)\n",
    "        square_error = tf.divide(square_error_numerator, square_error_denominator)\n",
    "                \n",
    "        log_p_x = -( square_error + tf.exp(sigma_x) )\n",
    "        log_p_x = tf.reduce_sum(log_p_x, axis=2)\n",
    "        log_p_x = tf.reduce_mean(log_p_x, axis=[0, 1])\n",
    "        \n",
    "        return log_p_x\n",
    "        \n",
    "    def kl_q_p(self, z, mu_log_sigma):\n",
    "        \n",
    "        b, n, k = z.shape\n",
    "        \n",
    "        #mu_q, log_sigma_q = mu_log_sigma[:, :-1], mu_log_sigma[:, -1]\n",
    "        #mu_q = tf.reshape(mu_q, (b, 1, k))\n",
    "        #sigma_q = tf.reshape(tf.exp(log_sigma_q), (b, 1, 1))\n",
    "        \n",
    "        mu_q, log_sigma_q = mu_log_sigma\n",
    "        sigma_q = tf.exp(log_sigma_q)\n",
    "        mu_q = tf.reshape(mu_q, (b, 1, k))\n",
    "        sigma_q = tf.reshape(sigma_q, (b, 1, k))\n",
    "        \n",
    "        log_p = -.5 * tf.square(z)\n",
    "        \n",
    "        log_q = -.5 * tf.square(z - mu_q)\n",
    "        log_q = tf.divide(log_q, tf.square(sigma_q))\n",
    "        log_q = log_q - tf.reshape(log_sigma_q, (b, 1, -1))\n",
    "                          \n",
    "        kl = tf.reduce_sum(log_q - log_p, axis=2)\n",
    "        return tf.reduce_mean(kl, axis=[0, 1])      \n",
    "        \n",
    "    def elbo(self, x, n=1):\n",
    "        \n",
    "        mu_log_sigma = self.encode(x)\n",
    "        z = self.sample_and_reparameterize(mu_log_sigma, n)\n",
    "        mu_x = self.decode(z)\n",
    "        \n",
    "        sigma_x = tf.exp(self.log_sig_x)\n",
    "        kl = self.kl_q_p(z, mu_log_sigma)\n",
    "        \n",
    "        return self.log_p_x(x, mu_x, sigma_x) - kl\n",
    "    \n",
    "    def compute_loss(self, x):\n",
    "        return -self.elbo(x)\n",
    "    \n",
    "    def train_step(self, x):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(x)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def test_step(self, x):\n",
    "        \n",
    "        loss = self.compute_loss(x)\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def call(self, inputs, is_training=False):\n",
    "        \n",
    "        inputs_is_list = isinstance(inputs, list)\n",
    "        \n",
    "        if inputs_is_list and is_training:\n",
    "            return [self.train_step(x_y) for x_y in inputs]\n",
    "        \n",
    "        elif inputs_is_list and not is_training:\n",
    "            return [self.test_step(x_y) for x_y in inputs]\n",
    "        \n",
    "        elif not inputs_is_list and is_training:\n",
    "            return self.train_step(inputs)\n",
    "        \n",
    "        elif not inputs_is_list and not is_training:\n",
    "            return self.test_step(inputs)\n",
    "        \n",
    "    def save(self, folder_path):\n",
    "        \n",
    "        self.encoder.save(folder_path + 'encoder')\n",
    "        self.decoder.save(folder_path + 'decoder')\n",
    "        \n",
    "        sig = self.log_sig_x.value().numpy()\n",
    "        np.save(folder_path + 'log_sig_x', sig)\n",
    "        \n",
    "    def load(self, folder_path):\n",
    "        \n",
    "        paths = [folder_path + f for f in os.listdir(folder_path)]\n",
    "        \n",
    "        encoder_path = None\n",
    "        decoder_path = None\n",
    "        log_sig_x_path = None\n",
    "        \n",
    "        for p in paths:\n",
    "            \n",
    "            if 'encoder' in p:\n",
    "                encoder_path = p\n",
    "            if 'decoder' in p:\n",
    "                decoder_path = p\n",
    "            if 'log_sig' in p:\n",
    "                log_sig_x_path = p\n",
    "        \n",
    "        log_sig_x = np.load(log_sig_x_path)            \n",
    "        \n",
    "        self.encoder = tf.keras.models.load_model(encoder_path)\n",
    "        self.decoder = tf.keras.models.load_model(decoder_path)\n",
    "        self.log_sig_x = tf.Variable(initial_value=log_sig_x, trainable=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9929d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae = MyVAE(LATENT_DIM, (BATCH_SIZE, 128, SEQUENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28955ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model for YTPiano z29, seq64\n",
    "#load_model_path = CVAE_DIR + 'z_29_seq_64_epochs_5_loss_9435.579/'\n",
    "\n",
    "# Best model for YTPiano z64, seq64\n",
    "load_model_path = CVAE_DIR + 'z_64_seq_64_epochs_5_loss_9864.694/'\n",
    "\n",
    "\n",
    "cvae.load(load_model_path)\n",
    "cvae.compile(tf.keras.optimizers.Adam(1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21a8709e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 128, 64)]    0           []                               \n",
      "                                                                                                  \n",
      " encoder_reshape (Reshape)      (None, 128, 64, 1)   0           ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " encoder_conv2d_1 (Conv2D)      (None, 32, 16, 64)   1088        ['encoder_reshape[0][0]']        \n",
      "                                                                                                  \n",
      " encoder_conv2d_2 (Conv2D)      (None, 8, 4, 128)    131200      ['encoder_conv2d_1[0][0]']       \n",
      "                                                                                                  \n",
      " encoder_conv2d_3 (Conv2D)      (None, 1, 2, 256)    524544      ['encoder_conv2d_2[0][0]']       \n",
      "                                                                                                  \n",
      " encoder_flatten (Flatten)      (None, 512)          0           ['encoder_conv2d_3[0][0]']       \n",
      "                                                                                                  \n",
      " encoder_mu (Dense)             (None, 64)           32832       ['encoder_flatten[0][0]']        \n",
      "                                                                                                  \n",
      " encoder_log_sigma (Dense)      (None, 64)           32832       ['encoder_flatten[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 722,496\n",
      "Trainable params: 722,496\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cvae.encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f716bd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 64)]              0         \n",
      "                                                                 \n",
      " decoder_dense (Dense)       (None, 512)               33280     \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 1, 2, 256)         0         \n",
      "                                                                 \n",
      " decoder_conv2dtranspose_1 (  (None, 8, 4, 128)        524416    \n",
      " Conv2DTranspose)                                                \n",
      "                                                                 \n",
      " decoder_conv2dtranspose_2 (  (None, 32, 16, 64)       131136    \n",
      " Conv2DTranspose)                                                \n",
      "                                                                 \n",
      " decoder_conv2dtranspose_3 (  (None, 128, 64, 1)       1025      \n",
      " Conv2DTranspose)                                                \n",
      "                                                                 \n",
      " decoder_output (Reshape)    (None, 128, 64)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 689,857\n",
      "Trainable params: 689,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cvae.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "48cb8cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean encode output: (32, 64)\n",
      "var encode output: (32, 64)\n",
      "sample/repar output: (32, 1, 64)\n",
      "decode output: (32, 1, 8192)\n",
      "sample_vect output: (32, 128, 64)\n",
      "logpx output: ()\n",
      "klqp output: ()\n",
      "elbo output: ()\n"
     ]
    }
   ],
   "source": [
    "test_encode_sample_output = cvae.encode(sample_input)\n",
    "print(f'mean encode output: {test_encode_sample_output[0].shape}')\n",
    "print(f'var encode output: {test_encode_sample_output[1].shape}')\n",
    "\n",
    "test_sar_sample_output = cvae.sample_and_reparameterize(test_encode_sample_output, 1)\n",
    "print(f'sample/repar output: {test_sar_sample_output.shape}')\n",
    "\n",
    "test_decode_sample_output = cvae.decode(test_sar_sample_output)\n",
    "print(f'decode output: {test_decode_sample_output.shape}')\n",
    "\n",
    "test_sample_vectors_sample_output = cvae.sample_latent_vectors(sample_input)\n",
    "print(f'sample_vect output: {test_sample_vectors_sample_output.shape}')\n",
    "\n",
    "test_logpx_sample_output = cvae.log_p_x(sample_input, tf.expand_dims(test_decode_sample_output, 1), cvae.log_sig_x)\n",
    "print(f'logpx output: {test_logpx_sample_output.shape}')\n",
    "\n",
    "test_klqp_sample_output = cvae.kl_q_p(test_sar_sample_output, test_encode_sample_output)\n",
    "print(f'klqp output: {test_klqp_sample_output.shape}')\n",
    "\n",
    "test_elbo_sample_output = cvae.elbo(sample_input)\n",
    "print(f'elbo output: {test_elbo_sample_output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2c94fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, model_dir, save_last_only):\n",
    "        \n",
    "        self.model_dir = model_dir\n",
    "        self.save_last_only = save_last_only\n",
    "        \n",
    "        self.best_loss = np.Inf\n",
    "        self.best_epoch = 0\n",
    "        self.best_model = None\n",
    "        self.num_epochs = None\n",
    "        \n",
    "        self.best_encoder = None\n",
    "        self.best_decoder = None\n",
    "        \n",
    "        self.latent_dim = None\n",
    "        self.sequence_length = None\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.num_epochs = self.params['epochs']\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "                \n",
    "        if self.save_last_only and epoch+1 == self.num_epochs:\n",
    "            \n",
    "            self.best_loss = logs['loss']\n",
    "            self.best_epoch = epoch + 1\n",
    "            self.best_model = self.model\n",
    "            \n",
    "            return\n",
    "            \n",
    "        elif logs['loss'] < self.best_loss:\n",
    "            \n",
    "            self.best_loss = logs['loss']\n",
    "            self.best_epoch = epoch + 1\n",
    "            self.best_model = self.model\n",
    "            \n",
    "            self.latent_dim = self.best_model.latent_dim\n",
    "            self.sequence_length = self.best_model.sequence_length\n",
    "            \n",
    "    def on_train_end(self, epoch_end_dict):\n",
    "        \n",
    "        self.best_loss = round(self.best_loss, 3)\n",
    "        folder_path = (self.model_dir + \n",
    "                       f'z_{self.latent_dim}_seq_{self.sequence_length}_' +\n",
    "                       f'epochs_{self.best_epoch}_loss_{self.best_loss}/'\n",
    "                      )\n",
    "        self.best_model.save(folder_path)\n",
    "    \n",
    "#vae_ckpt_clbk = MyModelCheckpointCallback(CVAE_DIR, False)\n",
    "vae_reduce_lr_clbk = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', min_delta=10, patience=4, factor=.1)\n",
    "vae_early_stop_clbk = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=20, patience=7)\n",
    "vae_save_final_model_clbk = MyModelCheckpointCallback(CVAE_DIR, True)\n",
    "\n",
    "vae_clbks = [#vae_ckpt_clbk, \n",
    "             vae_reduce_lr_clbk, \n",
    "             vae_early_stop_clbk,\n",
    "             vae_save_final_model_clbk\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5be469ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 134s 134ms/step - loss: 9818.4997 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 170s 170ms/step - loss: 9702.4352 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 193s 193ms/step - loss: 9630.4186 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 261s 262ms/step - loss: 9595.4754 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 238s 238ms/step - loss: 9560.5188 - lr: 0.0010\n",
      "INFO:tensorflow:Assets written to: C:/_local/py/classical_music_vae/models/cvae/z_64_seq_64_epochs_5_loss_9570.482/encoder\\assets\n",
      "INFO:tensorflow:Assets written to: C:/_local/py/classical_music_vae/models/cvae/z_64_seq_64_epochs_5_loss_9570.482/decoder\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d58e787430>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvae.fit(x=cvae_train_dataset, shuffle=False,\n",
    "         steps_per_epoch=1000, \n",
    "         epochs=5,\n",
    "         #validation_data=cvae_test_dataset, validation_steps=150,\n",
    "         callbacks=vae_clbks\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c91e30c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample # 29\n"
     ]
    }
   ],
   "source": [
    "play_samples_from_batch(sample_input, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5090f1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample # 31\n"
     ]
    }
   ],
   "source": [
    "sample_output = cvae.sample_latent_vectors(sample_input)\n",
    "play_samples_from_batch(sample_output, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41bd5989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample #31\n",
      "input\n",
      "output\n"
     ]
    }
   ],
   "source": [
    "play_inputs_and_outputs(sample_input, sample_output, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8313e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_piano_roll_batch_to_storage(sample_output, NOTE_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d4236d",
   "metadata": {},
   "source": [
    "# Training CVAE on sequence of notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6546e757",
   "metadata": {},
   "source": [
    "## Creating TF Datasets of sequences of latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cf5bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelodyTargetSequenceGenerator:\n",
    "    \n",
    "    def __init__(self, sparse_tensors, \n",
    "                 melody_sequence_length, melody_number_of_sequences, \n",
    "                 target_sequence_length, seed=None):\n",
    "        \n",
    "        self.sparse_tensors = sparse_tensors\n",
    "        self.num_tensors = len(self.sparse_tensors)\n",
    "        \n",
    "        self.melody_sequence_length = melody_sequence_length\n",
    "        self.melody_number_of_sequences = melody_number_of_sequences\n",
    "        self.melody_total_length = self.melody_sequence_length * self.melody_number_of_sequences\n",
    "        \n",
    "        self.target_sequence_length = target_sequence_length\n",
    "                \n",
    "        self.seed = seed\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            sparse_tensor = self.sparse_tensors[np.random.randint(0, self.num_tensors)]\n",
    "                                    \n",
    "            last_start = (sparse_tensor.shape[1] - self.melody_total_length - self.target_sequence_length - 3)\n",
    "            last_start -= last_start % 32\n",
    "            \n",
    "            melody_start = np.random.randint(0, last_start)\n",
    "            target_start = melody_start + self.melody_total_length\n",
    "            \n",
    "            melody = tf.sparse.slice(sparse_tensor,\n",
    "                                     start=[0, melody_start],\n",
    "                                     size=[128, self.melody_total_length]\n",
    "                                    )\n",
    "            \n",
    "            melody = tf.sparse.to_dense(melody)\n",
    "            \n",
    "            melody = tf.split(melody, axis=1, num_or_size_splits=self.melody_number_of_sequences)\n",
    "            melody = tf.stack(melody)\n",
    "            \n",
    "            target = tf.sparse.slice(sparse_tensor,\n",
    "                                     start=[0, target_start],\n",
    "                                     size=[128, self.target_sequence_length]\n",
    "                                    )\n",
    "            \n",
    "            target = tf.sparse.to_dense(target)\n",
    "            target = tf.expand_dims(target, 0)\n",
    "                \n",
    "            yield melody, target\n",
    "            #yield tf.squeeze(melody), target\n",
    "            \n",
    "                    \n",
    "    def __call__(self):\n",
    "        return self.__iter__()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adc3fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE_OF_GENERATORS = 50\n",
    "\n",
    "MELODY_NUMBER_OF_SEQUENCES = 6\n",
    "\n",
    "MELODY_TOTAL_LENGTH = MELODY_NUMBER_OF_SEQUENCES * SEQUENCE_LENGTH\n",
    "TARGET_TOTAL_LENGTH = SEQUENCE_LENGTH\n",
    "\n",
    "train_sub_generators = [MelodyTargetSequenceGenerator(\n",
    "                        PIANO_ROLLS_TRAIN[i*DATA_SIZE_OF_GENERATORS:(i+1)*DATA_SIZE_OF_GENERATORS], \n",
    "                        SEQUENCE_LENGTH, MELODY_NUMBER_OF_SEQUENCES, SEQUENCE_LENGTH, seed=i)\n",
    "                        \n",
    "                        for i in range(1 + PIANO_ROLLS_TRAIN.shape[0] // DATA_SIZE_OF_GENERATORS)\n",
    "                        if i*DATA_SIZE_OF_GENERATORS < PIANO_ROLLS_TRAIN.shape[0]]\n",
    "\n",
    "test_sub_generators = [MelodyTargetSequenceGenerator(\n",
    "                       PIANO_ROLLS_TEST[i*DATA_SIZE_OF_GENERATORS:(i+1)*DATA_SIZE_OF_GENERATORS], \n",
    "                       SEQUENCE_LENGTH, MELODY_NUMBER_OF_SEQUENCES, SEQUENCE_LENGTH, seed=i)\n",
    "                       \n",
    "                       for i in range(1 + PIANO_ROLLS_TEST.shape[0] // DATA_SIZE_OF_GENERATORS)\n",
    "                       if i*DATA_SIZE_OF_GENERATORS < PIANO_ROLLS_TEST.shape[0]]\n",
    "\n",
    "melody_gen_output_signature_roll = (tf.TensorSpec(shape=(MELODY_NUMBER_OF_SEQUENCES, 128, SEQUENCE_LENGTH)),\n",
    "                                    tf.TensorSpec(shape=(1, 128, SEQUENCE_LENGTH)))\n",
    "\n",
    "melody_gen_output_signature = melody_gen_output_signature_roll\n",
    "\n",
    "def get_sub_dataset(sub_generator, spec, batch_size, prefetch_size):\n",
    "    \n",
    "    return (tf.data.Dataset\n",
    "            .from_generator(sub_generator, output_signature=spec)\n",
    "            .batch(batch_size, drop_remainder=True)\n",
    "            .prefetch(prefetch_size)\n",
    "           )\n",
    "\n",
    "melody_train_sub_datasets = [get_sub_dataset(g, melody_gen_output_signature, BATCH_SIZE, 5)\n",
    "                          for g in train_sub_generators]\n",
    "melody_test_sub_datasets = [get_sub_dataset(g, melody_gen_output_signature, BATCH_SIZE, 5)\n",
    "                         for g in test_sub_generators]\n",
    "\n",
    "melody_train_dataset = tf.data.Dataset.sample_from_datasets(melody_train_sub_datasets).prefetch(64)\n",
    "melody_test_dataset = tf.data.Dataset.sample_from_datasets(melody_test_sub_datasets).prefetch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eff1ba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 895 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample_input = None\n",
    "for x in melody_train_dataset.take(1):\n",
    "    sample_input = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "704efb97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(32, 6, 128, 64), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(32, 1, 128, 64), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melody_train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "783fdc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111.00000005960464"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input[0].numpy().max()*127."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcad27",
   "metadata": {},
   "source": [
    "## Creating and training melody predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ff8f1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles encoding of sequences into latent vectors\n",
    "class VAEMelodyPredictior(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, latent_dim, batch_size,\n",
    "                 sequence_length, melody_number_of_sequences, \n",
    "                 encoder=None, decoder=None):\n",
    "        super(VAEMelodyPredictior, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.melody_number_of_sequences = melody_number_of_sequences\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.encoder.trainable = False\n",
    "        self.decoder.trainable = False\n",
    "        \n",
    "        # ---------------------------------------------- Prediction model ----------------------------------------------- #\n",
    "        \n",
    "        # MLP Definition        \n",
    "        \n",
    "        self.model = Sequential([\n",
    "            InputLayer(input_shape=(self.melody_number_of_sequences, self.latent_dim), name='mlp_input'),\n",
    "            Flatten(name='mlp_flatten'),\n",
    "            Dense(256, activation='relu', name='mlp_dense_1'),\n",
    "            Dense(256, activation='relu', name='mlp_dense_2'),\n",
    "            #Dropout(.2, name='mlp_dropout_1'),\n",
    "            Dense(256, activation='relu', name='mlp_dense_3'),\n",
    "            Dense(256, activation='relu', name='mlp_dense_4'),\n",
    "            #Dropout(.2, name='mlp_dropout_2'),\n",
    "            Dense(256, activation='relu', name='mlp_dense_5'),\n",
    "            Dense(self.latent_dim, activation='linear', name='mlp_output'),\n",
    "        ])\n",
    "        \n",
    "        self.model = Sequential([\n",
    "            \n",
    "        ])\n",
    "        \n",
    "    def compile(self, optimizer):\n",
    "        super(VAEMelodyPredictior, self).compile()\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.encoder.compile(self.optimizer, loss=None)\n",
    "        self.decoder.compile(self.optimizer, loss=None)\n",
    "            \n",
    "        self.model.compile(self.optimizer, loss=None)\n",
    "        \n",
    "    #def _compute_gradients(tensor, var_list):\n",
    "    #    grads = tf.gradients(tensor, var_list)\n",
    "        \n",
    "    #    return [grad if grad is not None else tf.zeros_like(var)\n",
    "    #            for var, grad in zip(var_list, grads)]\n",
    "        \n",
    "    def encode_batch(self, B):\n",
    "                \n",
    "        # B has shape batch_size x melody_num_sequences x 128 x sequence_length\n",
    "        # We apply encoding along batch dimension\n",
    "        \n",
    "        # Each encoding has shape [melody_num_sequences, latent_dim]\n",
    "        obs = [self.encoder(B[b])[0] for b in range(B.shape[0])]\n",
    "        \n",
    "        # Returns tensor shaped [batch_size, melody_num_sequences, latent_dim]\n",
    "        return tf.stack(obs)\n",
    "        \n",
    "    def compute_loss(self, x_y):\n",
    "        \n",
    "        x, y = x_y\n",
    "        \n",
    "        # Get encodings for current sequence\n",
    "        x_latent_vectors = tf.vectorized_map(lambda x: self.encoder(x)[0], x, fallback_to_while_loop=False)\n",
    "                \n",
    "        # Predict encoding of next sequence based on current sequence encoding\n",
    "        y_latent_vectors_pred = self.model(x_latent_vectors) # <-- [batch_size, latent_dim]\n",
    "                \n",
    "        # Get encodings of next sequence (label)\n",
    "        y_latent_vectors = tf.squeeze(tf.vectorized_map(lambda y: self.encoder(y)[0], y, fallback_to_while_loop=False))\n",
    "                        \n",
    "        # MSE between encodings\n",
    "        return tf.keras.losses.MeanSquaredError()(y_latent_vectors, y_latent_vectors_pred)\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, x_y):\n",
    "        \n",
    "        # x has shape batch_size x melody_num_sequences x 128 x sequence_length\n",
    "        # y has shape batch_size x 128 x sequence_length\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(x_y)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return {'mse': loss}\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, x_y):\n",
    "        \n",
    "        loss = self.compute_loss(x_y)\n",
    "        return {'mse': loss}\n",
    "    \n",
    "    def call(self, inputs, is_training=False):\n",
    "        \n",
    "        inputs_is_list = isinstance(inputs, list)\n",
    "        \n",
    "        if inputs_is_list and is_training:\n",
    "            return [self.train_step(x_y) for x_y in inputs]\n",
    "        \n",
    "        elif inputs_is_list and not is_training:\n",
    "            return [self.test_step(x_y) for x_y in inputs]\n",
    "        \n",
    "        elif not inputs_is_list and is_training:\n",
    "            return self.train_step(x_y)\n",
    "        \n",
    "        elif not inputs_is_list and not is_training:\n",
    "            return self.test_step(x_y)\n",
    "        \n",
    "    def predict_piano_roll(self, x_piano_rolls):\n",
    "        \n",
    "        # Get piano roll encoding\n",
    "        x_latent = self.encode_batch(x_piano_rolls)\n",
    "        \n",
    "        # Predict encoding of next piano roll in song\n",
    "        y_pred_latent = self.model(x_latent)\n",
    "         \n",
    "        # Decode predicted encoding\n",
    "        piano_roll_pred = self.decoder(y_pred_latent).numpy()\n",
    "            \n",
    "        piano_roll_pred = piano_roll_pred.reshape((x_piano_rolls.shape[0], 128, self.sequence_length))\n",
    "        piano_roll_pred[piano_roll_pred > 1] = 1\n",
    "        piano_roll_pred[piano_roll_pred < 0] = 0\n",
    "        \n",
    "        return piano_roll_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "54ff71dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "melody_vae = VAEMelodyPredictior(LATENT_DIM, BATCH_SIZE, SEQUENCE_LENGTH, MELODY_NUMBER_OF_SEQUENCES,\n",
    "                                 cvae.encoder, cvae.decoder)\n",
    "\n",
    "melody_vae.compile(tf.keras.optimizers.Adam(1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "20efc6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mlp_flatten (Flatten)       (None, 174)               0         \n",
      "                                                                 \n",
      " mlp_dense_1 (Dense)         (None, 256)               44800     \n",
      "                                                                 \n",
      " mlp_dense_2 (Dense)         (None, 256)               65792     \n",
      "                                                                 \n",
      " mlp_dense_3 (Dense)         (None, 256)               65792     \n",
      "                                                                 \n",
      " mlp_dense_4 (Dense)         (None, 256)               65792     \n",
      "                                                                 \n",
      " mlp_dense_5 (Dense)         (None, 256)               65792     \n",
      "                                                                 \n",
      " mlp_output (Dense)          (None, 29)                7453      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 315,421\n",
      "Trainable params: 315,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "melody_vae.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d0d52acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelodyVaeCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, model_dir, save_last_only):\n",
    "        \n",
    "        self.model_dir = model_dir        \n",
    "        self.save_last_only = save_last_only\n",
    "        \n",
    "        self.latent_dim = None\n",
    "        self.sequence_length = None\n",
    "        self.melody_number_of_sequences = None\n",
    "        \n",
    "        self.best_loss = np.Inf\n",
    "        self.best_epoch = None\n",
    "        self.best_model = None\n",
    "        \n",
    "        self.num_epochs = None\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        \n",
    "        self.latent_dim = self.model.latent_dim\n",
    "        self.sequence_length = self.model.sequence_length\n",
    "        self.melody_number_of_sequences = self.model.melody_number_of_sequences\n",
    "        \n",
    "        self.num_epochs = self.params['epochs']\n",
    "        \n",
    "    def on_epoch_end(self, epoch, loss_dict):            \n",
    "        \n",
    "        if self.save_last_only and epoch+1 == self.num_epochs:\n",
    "            \n",
    "            self.best_loss = loss_dict['mse']\n",
    "            self.best_epoch = epoch + 1\n",
    "            self.best_model = self.model.model\n",
    "            \n",
    "            return\n",
    "                        \n",
    "        if loss_dict['mse'] < self.best_loss:\n",
    "                        \n",
    "            self.best_epoch = epoch + 1\n",
    "            self.best_loss = loss_dict['mse']   \n",
    "            \n",
    "            self.best_model = self.model.model\n",
    "    \n",
    "    def on_train_end(self, loss_dict):\n",
    "        \n",
    "        model_file_name = (f'seq_{self.sequence_length}_z_{self.latent_dim}_mel_{self.melody_number_of_sequences}_'\n",
    "                           f'epochs_{self.best_epoch}_mse_{round(self.best_loss, 3)}'\n",
    "                          )\n",
    "        model_file_path = self.model_dir + model_file_name\n",
    "        \n",
    "        self.best_model.save(model_file_path)   \n",
    "\n",
    "melody_vae_ckpt_clbk = MelodyVaeCheckpointCallback(MELODY_PREDICTION_MODEL_DIR, True)\n",
    "melody_vae_reduce_lr_clbk = tf.keras.callbacks.ReduceLROnPlateau(monitor='mse', patience=4, min_delta=.01, factor=.1)\n",
    "melody_vae_early_stop_clbk = tf.keras.callbacks.EarlyStopping(monitor='mse', patience=6, min_delta=.01)\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    \n",
    "    if epoch+1 % 3 != 0:\n",
    "        return lr\n",
    "    \n",
    "    else:\n",
    "        return lr * tf.math.exp(-.05)\n",
    "    \n",
    "melody_vae_lr_schedule = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "melody_vae_callbacks = [melody_vae_ckpt_clbk, \n",
    "                        melody_vae_reduce_lr_clbk, \n",
    "                        melody_vae_early_stop_clbk,\n",
    "                        #melody_vae_lr_schedule\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ebcca002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 95s 95ms/step - mse: 0.6260 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 117s 117ms/step - mse: 0.5571 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 114s 114ms/step - mse: 0.5443 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "1000/1000 [==============================] - 130s 130ms/step - mse: 0.5380 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "1000/1000 [==============================] - 121s 121ms/step - mse: 0.5348 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "1000/1000 [==============================] - 128s 129ms/step - mse: 0.5382 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "1000/1000 [==============================] - 143s 143ms/step - mse: 0.5084 - lr: 1.0000e-04\n",
      "Epoch 8/15\n",
      "1000/1000 [==============================] - 129s 129ms/step - mse: 0.5038 - lr: 1.0000e-04\n",
      "Epoch 9/15\n",
      "1000/1000 [==============================] - 121s 121ms/step - mse: 0.4948 - lr: 1.0000e-04\n",
      "Epoch 10/15\n",
      "1000/1000 [==============================] - 135s 135ms/step - mse: 0.4997 - lr: 1.0000e-04\n",
      "Epoch 11/15\n",
      "1000/1000 [==============================] - 183s 183ms/step - mse: 0.4947 - lr: 1.0000e-04\n",
      "Epoch 12/15\n",
      "1000/1000 [==============================] - 185s 185ms/step - mse: 0.4980 - lr: 1.0000e-05\n",
      "Epoch 13/15\n",
      "1000/1000 [==============================] - 180s 180ms/step - mse: 0.4970 - lr: 1.0000e-05\n",
      "INFO:tensorflow:Assets written to: C:/_local/py/classical_music_vae/models/melody_predictor/seq_64_z_29_mel_6_epochs_7_mse_0.314\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b3751fae50>"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melody_vae.fit(x=melody_train_dataset, shuffle=False,\n",
    "               epochs=15, steps_per_epoch=1000,\n",
    "               #validation_data=melody_test_dataset, validation_steps=150,\n",
    "               callbacks=melody_vae_callbacks\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "da9be712",
   "metadata": {},
   "outputs": [],
   "source": [
    "MELODY_VAE_MODEL_DIR = 'C:/_local/py/yt_piano_music_gen/models/melody_predictor/seq_32_z_128_mel_4_epochs_3_mse_0.22'\n",
    "\n",
    "#melody_vae.model = tf.keras.models.load_model(MELODY_VAE_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8055f",
   "metadata": {},
   "source": [
    "## Generating music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "31e3763b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample # 31\n"
     ]
    }
   ],
   "source": [
    "play_samples_from_batch(sample_input[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "7d0ddec1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_output_batch = melody_vae.predict_piano_roll(sample_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f36cf0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample output shape: (32, 128, 64)\n",
      "sample output max: 0.6117526888847351\n",
      "sample output min: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f'sample output shape: {sample_output_batch.shape}')\n",
    "print(f'sample output max: {sample_output_batch.max()}')\n",
    "print(f'sample output min: {sample_output_batch.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "c57d5eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample # 31\n"
     ]
    }
   ],
   "source": [
    "play_samples_from_batch(sample_output_batch, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "7c3044fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_piano_roll = sample_input[0][31]\n",
    "sample_output = melody_vae.predict_piano_roll(tf.expand_dims(sample_piano_roll, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "82249b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([6, 128, 64])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_piano_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "75737a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_piano_roll = concatenate_piano_rolls(sample_piano_roll, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "6414e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 384)\n",
      "0.72440946\n"
     ]
    }
   ],
   "source": [
    "print(sample_piano_roll.shape)\n",
    "print(sample_piano_roll.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c57498f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 64)\n",
      "0.0\n",
      "0.56114995\n"
     ]
    }
   ],
   "source": [
    "print(sample_output.shape)\n",
    "print(sample_output.min())\n",
    "print(sample_output.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "085aeeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "play_piano_roll(sample_piano_roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "37ff9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_piano_roll(sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "58603cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_song = concatenate_piano_rolls([sample_piano_roll, sample_output], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "bbc4bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "play_piano_roll(sample_song, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5b60d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_song_from_input(input_array, model, sequence_length, number_of_sequences, zero_buf):\n",
    "    \n",
    "    if isinstance(input_array, tf.Tensor):\n",
    "        input_piano_roll = input_array.numpy().copy()\n",
    "    else:\n",
    "        input_piano_roll = input_array.copy()\n",
    "            \n",
    "    assert input_piano_roll.shape[-1] == sequence_length, 'inputs sequence length does not match sequence length'\n",
    "    assert input_piano_roll.shape[-2] == 128, 'input piano roll does not have 128 notes'\n",
    "        \n",
    "    if len(input_piano_roll.shape) == 2:\n",
    "        input_piano_roll = np.expand_dims(input_piano_roll, 0)\n",
    "    \n",
    "    input_piano_roll[input_piano_roll > 1] = 1\n",
    "    input_piano_roll[input_piano_roll < 0] = 0\n",
    "    \n",
    "    flat_input_piano_roll = [input_piano_roll[p] for p in range(input_piano_roll.shape[0])]\n",
    "    flat_input_piano_roll = np.concatenate(flat_input_piano_roll, axis=-1)\n",
    "           \n",
    "    total_number_of_sequences = input_piano_roll.shape[0] + number_of_sequences\n",
    "    \n",
    "    piano_roll = np.zeros(shape=(total_number_of_sequences, 128, sequence_length))\n",
    "    \n",
    "    x_start_index = 0\n",
    "    y_index = input_piano_roll.shape[0]\n",
    "    \n",
    "    piano_roll[x_start_index:y_index, :, :] = input_piano_roll\n",
    "    \n",
    "    piano_roll_list = [flat_input_piano_roll]\n",
    "        \n",
    "    for n in range(number_of_sequences):\n",
    "                \n",
    "        x = np.expand_dims(piano_roll[x_start_index:y_index, :, :], 0) \n",
    "        y_pred = model.predict_piano_roll(x)\n",
    "        \n",
    "        piano_roll[y_index, :, :] = y_pred\n",
    "        piano_roll_list.append(y_pred)\n",
    "        \n",
    "        x_start_index += 1\n",
    "        y_index += 1\n",
    "    \n",
    "    return concatenate_piano_rolls(piano_roll_list, zero_buf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "a81e833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_song = generate_song_from_input(sample_input[0][17], melody_vae, SEQUENCE_LENGTH, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "3589cf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 266)\n",
      "0.0018742169486358762\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(sample_song[:, 128*6:].shape)\n",
    "print(sample_song[:, 128*6:].max())\n",
    "print(sample_song[:, 128*6:].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "eada313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_piano_roll(sample_song)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
